Inference:
    Models:
        Seq2SeqwithXfmrMemEfficient:
            predMaxLen: 150
            encMaxLen: 4000
            embMult: 4
    OtherParams:
        padToken: 0
        seed: 0
        fullVocab: true #Use the full vocab or generate vocab based only upon the data used for training
        cpcCodes: "de"
        fname: "data0_str_json.gz"
        trainSize: 500
        valSize: 16


Train:
    Models:
        Seq2SeqwithXfmrMemEfficient:
            predMaxLen: 150
            encMaxLen: 4000
            embMult: 4
    OtherParams:
        padToken: 0
        seed: 0
        fullVocab: true #Use the full vocab or generate vocab based only upon the data used for training
        cpcCodes: "de"
        fname: "data0_str_json.gz"
        trainSize: 5000
        valSize: 16
        printEveryIters: 500
        l2Reg: 0.0
        toTrain: true #Flag to train or evaluate the model
        loadFromArtifact: true #false #true


Unit_Tests:
    Models:
        Seq2SeqwithXfmrMemEfficient:
            predMaxLen: 150
            encMaxLen: 4000 #800
            hiddenDim: 128 #8
            embMult: 4
            numHeads: 4
            dropout: 0.0
            numLayers: 2 #6
            decNumLayers: 4 #8
            targetLen: 4300 #for assertion in test_models.py
            evalPredLen: 150 #for seed 0
    OtherParams:
        padToken: 0
        seed: 0
        batchSize: 2
        vocabSize: 10
        beamSize: 0 #3
        fullVocab: true
        cpcCodes: "de"
        fname: "data0_str_json.gz"
        trainSize: 4
        valSize: 4
        numEpochs: 2
        lr: 0.001
        printEveryIters: 2
        l2Reg: 0.0
        savedModelBaseName: MODEL1 #used for training
        loadModelName: 'MODEL1_best.pt' #used for inference
        modelType: 'Seq2SeqwithXfmrMemEfficient'
        
