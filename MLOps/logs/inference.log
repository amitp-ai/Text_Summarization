05.31.21 09:08:08:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:08:08:inference:Description Data shape is: [1, 97]
05.31.21 09:08:08:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary']}
05.31.21 09:08:09:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:08:09:inference:Starting model inference for the loaded model...
05.31.21 09:08:09:inference:	Model inference...
05.31.21 09:08:09:inference:torch.Size([1, 97])
05.31.21 09:08:10:inference:torch.Size([1, 71])
05.31.21 09:08:10:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:08:10:inference:
******************************************************************************

05.31.21 09:08:39:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:08:39:inference:Description Data shape is: [1, 97]
05.31.21 09:08:39:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:08:39:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'It', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:08:39:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:08:39:inference:Starting model inference for the loaded model...
05.31.21 09:08:39:inference:	Model inference...
05.31.21 09:08:39:inference:torch.Size([1, 97])
05.31.21 09:08:41:inference:torch.Size([1, 71])
05.31.21 09:08:41:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:08:41:inference:
******************************************************************************

05.31.21 09:09:08:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:09:13:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:09:17:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:09:48:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:09:57:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:10:14:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:10:22:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:10:22:inference:Description Data shape is: [1, 97]
05.31.21 09:10:22:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary']}
05.31.21 09:10:22:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:10:22:inference:Starting model inference for the loaded model...
05.31.21 09:10:22:inference:	Model inference...
05.31.21 09:10:22:inference:torch.Size([1, 97])
05.31.21 09:10:24:inference:torch.Size([1, 71])
05.31.21 09:10:24:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:10:24:inference:
******************************************************************************

05.31.21 09:10:43:inference:Namespace(batchSize=24, beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:10:43:inference:Description Data shape is: [1, 97]
05.31.21 09:10:43:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:10:43:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'It', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:10:44:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:10:44:inference:Starting model inference for the loaded model...
05.31.21 09:10:44:inference:	Model inference...
05.31.21 09:10:44:inference:torch.Size([1, 97])
05.31.21 09:10:45:inference:torch.Size([1, 71])
05.31.21 09:10:45:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:10:45:inference:
******************************************************************************

05.31.21 09:13:00:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:13:00:inference:Description Data shape is: [1, 97]
05.31.21 09:13:00:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:13:00:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'It', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:13:01:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:13:01:inference:Starting model inference for the loaded model...
05.31.21 09:13:01:inference:	Model inference...
05.31.21 09:13:01:inference:torch.Size([1, 97])
05.31.21 09:13:02:inference:torch.Size([1, 71])
05.31.21 09:13:02:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:13:02:inference:
******************************************************************************

05.31.21 09:13:09:inference:Namespace(beamSize=3, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:13:09:inference:Description Data shape is: [1, 97]
05.31.21 09:13:09:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:13:09:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'This', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'The', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'It', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:13:09:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:13:09:inference:Starting model inference for the loaded model...
05.31.21 09:13:09:inference:	Model inference...
05.31.21 09:13:09:inference:torch.Size([1, 97])
05.31.21 09:13:13:inference:torch.Size([1, 71])
05.31.21 09:13:13:inference:Prediction is
--start-- modular useful useful double storage and using traditional scaffold . the storage salts cooperate to previously available sized of large acids , plugged ropes . the present fluid glass can be available acids with utility assigned to various systems for suspending . they deformable capped now design , various systems at intervals to various application to a certain magnet such utilizes suitable for light , sodium by ropes . --stop--
05.31.21 09:13:13:inference:
******************************************************************************

05.31.21 09:15:20:inference:Namespace(beamSize=3, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:15:20:inference:Description Data shape is: [1, 97]
05.31.21 09:15:20:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:15:20:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:15:20:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:15:20:inference:Starting model inference for the loaded model...
05.31.21 09:15:20:inference:	Model inference...
05.31.21 09:15:20:inference:torch.Size([1, 97])
05.31.21 09:15:22:inference:torch.Size([1, 27])
05.31.21 09:15:22:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
05.31.21 09:15:22:inference:
******************************************************************************

05.31.21 09:17:55:inference:Namespace(beamSize=3, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
05.31.21 09:17:55:inference:Description Data shape is: [1, 97]
05.31.21 09:17:55:inference:Target Summary Data shape is: [1, 15]
05.31.21 09:17:55:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
05.31.21 09:17:55:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
05.31.21 09:17:55:inference:Starting model inference for the loaded model...
05.31.21 09:17:55:inference:	Model inference...
05.31.21 09:17:55:inference:torch.Size([1, 97])
05.31.21 09:17:57:inference:torch.Size([1, 27])
05.31.21 09:17:57:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
05.31.21 09:17:57:inference:
******************************************************************************

06.01.21 02:53:46:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 02:53:47:inference:Description Data shape is: [1, 97]
06.01.21 02:53:47:inference:Target Summary Data shape is: [1, 15]
06.01.21 02:53:47:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 02:53:49:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 02:53:49:inference:Starting model inference for the loaded model...
06.01.21 02:53:49:inference:	Model inference...
06.01.21 02:53:49:inference:torch.Size([1, 97])
06.01.21 02:53:49:inference:torch.Size([1, 27])
06.01.21 02:53:49:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 02:53:49:inference:
******************************************************************************

06.01.21 02:55:36:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 02:55:36:inference:Description Data shape is: [1, 97]
06.01.21 02:55:36:inference:Target Summary Data shape is: [1, 15]
06.01.21 02:55:36:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 02:55:36:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 02:55:36:inference:Starting model inference for the loaded model...
06.01.21 02:55:36:inference:	Model inference...
06.01.21 02:55:36:inference:torch.Size([1, 97])
06.01.21 02:55:36:inference:torch.Size([1, 27])
06.01.21 02:55:36:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 02:55:36:inference:
******************************************************************************

06.01.21 02:58:45:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 02:59:18:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 02:59:18:inference:Description Data shape is: [1, 97]
06.01.21 02:59:18:inference:Target Summary Data shape is: [1, 15]
06.01.21 02:59:18:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 02:59:18:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 02:59:18:inference:Starting model inference for the loaded model...
06.01.21 02:59:18:inference:	Model inference...
06.01.21 02:59:18:inference:torch.Size([1, 97])
06.01.21 02:59:19:inference:torch.Size([1, 27])
06.01.21 02:59:19:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 02:59:19:inference:
******************************************************************************

06.01.21 20:07:26:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:07:26:inference:Description Data shape is: [1, 97]
06.01.21 20:07:26:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:07:26:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:07:28:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:07:28:inference:Starting model inference for the loaded model...
06.01.21 20:07:28:inference:	Model inference...
06.01.21 20:07:28:inference:torch.Size([1, 97])
06.01.21 20:07:28:inference:torch.Size([1, 27])
06.01.21 20:07:28:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:22:29:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:22:29:inference:Description Data shape is: [1, 97]
06.01.21 20:22:29:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:22:29:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:22:29:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:22:29:inference:Starting model inference for the loaded model...
06.01.21 20:22:29:inference:	Model inference...
06.01.21 20:22:29:inference:torch.Size([1, 97])
06.01.21 20:22:29:inference:torch.Size([1, 27])
06.01.21 20:22:29:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:23:23:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:23:24:inference:Description Data shape is: [1, 97]
06.01.21 20:23:24:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:23:24:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:23:24:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:23:24:inference:Starting model inference for the loaded model...
06.01.21 20:23:24:inference:	Model inference...
06.01.21 20:23:24:inference:torch.Size([1, 97])
06.01.21 20:23:24:inference:torch.Size([1, 27])
06.01.21 20:23:24:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:23:24:inference:
******************************************************************************

06.01.21 20:23:57:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:23:57:inference:Description Data shape is: [1, 97]
06.01.21 20:23:57:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:23:57:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:23:58:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:23:58:inference:Starting model inference for the loaded model...
06.01.21 20:23:58:inference:	Model inference...
06.01.21 20:23:58:inference:torch.Size([1, 97])
06.01.21 20:23:58:inference:torch.Size([1, 27])
06.01.21 20:23:58:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:23:58:inference:
******************************************************************************

06.01.21 20:25:51:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:25:51:inference:Description Data shape is: [1, 97]
06.01.21 20:25:51:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:25:51:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:25:52:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:25:52:inference:Starting model inference for the loaded model...
06.01.21 20:25:52:inference:	Model inference...
06.01.21 20:25:52:inference:torch.Size([1, 97])
06.01.21 20:25:52:inference:torch.Size([1, 27])
06.01.21 20:25:52:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:25:52:inference:
******************************************************************************

06.01.21 20:29:29:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:29:29:inference:Description Data shape is: [1, 97]
06.01.21 20:29:29:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:29:29:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:29:30:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:29:30:inference:Starting model inference for the loaded model...
06.01.21 20:29:30:inference:	Model inference...
06.01.21 20:29:30:inference:torch.Size([1, 97])
06.01.21 20:29:30:inference:torch.Size([1, 27])
06.01.21 20:29:30:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:29:30:inference:
******************************************************************************

06.01.21 20:30:22:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:30:22:inference:Description Data shape is: [1, 97]
06.01.21 20:30:22:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:30:22:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:31:06:inference:Namespace(beamSize=0, configPath='./config.yaml', decNumLayers=4, dropout=0.0, hiddenDim=128, inputTextFile='inferenceData.json', loadModelName='MODEL7_step_20500.pth.tar', modelType='Seq2SeqwithXfmrMemEfficient', numHeads=4, numLayers=2)
06.01.21 20:31:06:inference:Description Data shape is: [1, 97]
06.01.21 20:31:06:inference:Target Summary Data shape is: [1, 15]
06.01.21 20:31:06:inference:{'Desc_Orig': 'this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary. this is some input text that needs to be summarized by the model. This text should first be preprocessed and then it is going to be preprocessed and then it will be fed to the model. The output of the model is going to be the text summary', 'Desc_AfterPreProcess': ['this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary.', 'this', 'is', 'some', 'input', 'text', 'that', 'needs', 'to', 'be', 'summarized', 'by', 'the', 'model.', 'this', 'text', 'should', 'first', 'be', 'preprocessed', 'and', 'then', 'it', 'is', 'going', 'to', 'be', 'preprocessed', 'and', 'then', 'it', 'will', 'be', 'fed', 'to', 'the', 'model.', 'the', 'output', 'of', 'the', 'model', 'is', 'going', 'to', 'be', 'the', 'text', 'summary'], 'TgtSmry_Original': 'this is the target summary . It should be treated as such .', 'TgtSmry_AfterPreProcess': ['this', 'is', 'the', 'target', 'summary', '.', 'it', 'should', 'be', 'treated', 'as', 'such', '.']}
06.01.21 20:31:06:inference:Loaded MODEL7_step_20500.pth.tar model for Seq2SeqwithXfmrMemEfficient, which is from step 20500 and metric value is 0.384
06.01.21 20:31:06:inference:Starting model inference for the loaded model...
06.01.21 20:31:06:inference:	Model inference...
06.01.21 20:31:06:inference:torch.Size([1, 97])
06.01.21 20:31:06:inference:torch.Size([1, 27])
06.01.21 20:31:06:inference:Prediction is
--start-- double storage and invention provides individuals useful to valuable packaging both catch periodically convenient that can be used to valuable found at their structures which --stop--
06.01.21 20:31:06:inference:
******************************************************************************

